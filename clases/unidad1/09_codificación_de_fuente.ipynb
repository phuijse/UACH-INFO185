{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoría de la Información - Parte 1\n",
    "\n",
    "En esta lección veremos:\n",
    "\n",
    "- Cantidad de información y entropía\n",
    "- Codificación de fuente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Codificación de fuente\n",
    "\n",
    "La codificación de fuente es el proceso que asigna un código a cada símbolo del diccionario\n",
    "\n",
    "**Ejemplo:** Queremos codificar las letras del alfabeto (que son 27) usando código binario\n",
    "\n",
    "Si usamos un código de largo fijo necesitariamos al menos 5 bits: $2^5 = 32 > 27$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "with open('../data/quijote.txt', 'r') as file:\n",
    "    texto = file.read()\n",
    "\n",
    "texto = texto.translate({ord(k): None for k in string.digits})\n",
    "#texto = texto.lower().encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(f\"Largo del texto: {len(texto)} caracteres\")\n",
    "print(f\"Cantidad de bits si uso 5 bits por símbolo: {len(texto)*5}\")\n",
    "print(texto[:101] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero algunas letras se ocupan más que otras \n",
    "\n",
    "El iterador `Counter` del módulo `collections` nos permite hacer un histograma de los caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(texto).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Intuición\n",
    ":class: tip\n",
    "Podríamos reducir la cantidad de bits si usamos códigos más cortos para las letras más frecuentes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introducción a la Teoría de la información\n",
    "\n",
    "La Teoría de la información TI es el estudio matemático sobre la cuantificación y transmisión de la información. Fue propuesto por **[Claude Shannon](https://es.wikipedia.org/wiki/Claude_Shannon)** en 1948: *A Mathematical Theory of Communication*. \n",
    "\n",
    "TI proporciona formas para describir la información de un proceso y tiene importantes aplicaciones en telecomunicaciones, computación y biología (genética). También ha tenido una fuerte influencia en la teoría de codificación y compresión.\n",
    "\n",
    "Partamos con un ejemplo\n",
    "\n",
    "**Ejemplo:** Las dos fuentes\n",
    "\n",
    "Sean dos fuentes **F1** y **F2** que pueden emitir uno entre cuatro símbolos: $A$, $B$, $C$ o $D$\n",
    "\n",
    "**F1** es completamente aleatoria, es decir: $P(A) = P(B) = P(C) = P(D) = \\frac{1}{4}$\n",
    "\n",
    "Si queremos predecir el próximo valor emitido por **F1** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer?\n",
    "\n",
    "<img src=\"../images/information1.svg\" width=\"600\">\n",
    "\n",
    "> La respuesta es 2 para cualquiera de los símbolos\n",
    "\n",
    "**F2** en cambio emite $A$, $B$, $C$ y $D$ con probabilidades $P(A) =\\frac{1}{2}$, $P(B) =\\frac{1}{4}$, $P(C) = \\frac{1}{8}$ y $P(D) =\\frac{1}{8}$, respectivamente\n",
    "\n",
    "Si queremos predecir el próximo valor retornado por **F2** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer? \n",
    "\n",
    "<img src=\"../images/information2.svg\" width=\"800\">\n",
    "\n",
    "> La respuesta es 1 para $A$, 2 para $B$ y 3 para $C$ y $D$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de información (según Shannon)\n",
    "\n",
    "La cantidad de información de un símbolo $x$ es el logaritmo en base dos del recíproco de su probabilidad de aparición\n",
    "\n",
    "$$\n",
    "I(x) = \\log_2 \\left(\\frac{1}{P(x)} \\right) = \\log_2 P(x)^{-1} = - \\log_2 P(x),\n",
    "$$\n",
    "\n",
    "que es equivalente a la mínima cantidad de preguntas si/no que debemos hacer para adivinar su valor\n",
    "\n",
    "La cantidad de información se mide en **bits**\n",
    "\n",
    ">Un **bit** es la cantidad de información que se requiere para escoger entre **dos** alternativas equiprobables\n",
    "\n",
    "La cantidad de información es también llamada **sorpresa**\n",
    "\n",
    "> Mientras más improbable es un símbolo, más nos sorprendemos cuando observamos que ocurre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía\n",
    "\n",
    "Sea una variable aleatoria $X$ (fuente) con $N$ resultados posibles (símbolos) $\\{x_1, x_2, \\ldots, x_N\\}$ donde cada símbolo $x_i$ tiene una probabilidad $p_i \\in [0, 1]$ y $\\sum_{i=1}^N p_i = 1$ \n",
    "\n",
    "Por ende cada símbolo tiene una cantidad de información  $I(x_i) = -\\log_2 \\left( p_i \\right)$ \n",
    "\n",
    "Definimos la **cantidad de información promedio** de $X$ como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H (X) &= \\mathbb{E}_{x\\sim X} \\left [ - \\log P(X=x) \\right ]  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N P(X=x_i) \\log_2 P(X=x_i)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i  \\quad \\text{[bits/símbolo]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "que se conoce como **Entropía de Shannon** \n",
    "\n",
    "**Propiedades de la entropía**\n",
    "\n",
    "- La entropía es siempre positiva $H(X) \\geq 0$. La igualdad se cumple si un $x_i$ tiene $p_i=1$ (caso más predecible)\n",
    "- La entropia está acotada $H(X) \\leq H_0$, donde $H_0= \\log_2(N)$ es la entropia si $p_i = \\frac{1}{N}~ \\forall i$ (caso menos predecible)\n",
    "- La redundancia de $X$ es $1 - H(X)/H_0$\n",
    "\n",
    "```{note}\n",
    "Mientras más predecible es $X$ menor es su entropía y mayor es su redundancia\n",
    "```\n",
    "\n",
    "\n",
    "**Ejemplo:** El retorno de las dos fuentes\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F1**?\n",
    "\n",
    "> $1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} = 2$ preguntas por símbolo. Su entropía es $2$ [bits]\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F2**?\n",
    "\n",
    "> $1 \\frac{1}{2} + 2 \\frac{1}{4} + 3 \\frac{1}{8} + 3 \\frac{1}{8} = 1.75$ preguntas por símbolo. Su entropía es $1.75$ [bits]\n",
    "\n",
    "Si cada fuente retorna un mensaje de 100 símbolos ¿Cúanta información produjo cada una?\n",
    "\n",
    "> **F1** produce 200 bits mientras que **F2** produce 175 bits\n",
    "\n",
    "Mientras más predecible menos información se necesita\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ejemplo:** Moneda con truco\n",
    "\n",
    "Sea una variable aleatoria $X$ que modela el resultado de lanzar una moneda y asumamos que el resultado puede tomar solo dos valores: Cara $o$ o Cruz $x$\n",
    "\n",
    "- La probabilidad de que salga cara es $p_o = p$\n",
    "- La probabilidad de que salga cruz es $p_x = 1- p$\n",
    "\n",
    "\n",
    "Luego la entropía es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -\\sum_{i=1}^2 p_i \\log_2 p_i \\nonumber \\\\ \n",
    "&= -p_x \\log (p_x) - p_o \\log p_o \\nonumber \\\\\n",
    "&= - p \\log(p) - (1-p) \\log(1-p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reflexione:\n",
    "\n",
    "- ¿En que casos la entropía es mínima? ¿En qué caso es máxima?\n",
    "- ¿Puedes relacionar la entropía con la aleatoridad/sorpresa del resultado de lanzar la moneda?\n",
    "\n",
    "\n",
    "Ojo: $\\lim_{z\\to 0^+} z \\log 1/z = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = np.linspace(0.01, 0.99, num=100)\n",
    "H = -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "fig, ax = plt.subplots(1, figsize=(6, 4), sharex=True)\n",
    "ax.set_xlabel('p')\n",
    "ax.plot(p, -np.log2(p), label='I(o)', lw=3)\n",
    "ax.plot(p, -np.log2(1-p), label='I(x)', lw=3)\n",
    "ax.plot(p, H, label='H(X)', lw=3)\n",
    "ax.set_ylim([0, 3])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "\n",
    "Sea una fuente que escupe un entero x que está entre 0 y 31\n",
    "\n",
    "Considere el resultado de las siguientes preguntas ¿Cúal tiene mayor entropía?\n",
    "- ¿Es x igual a 0?\n",
    "- ¿Es x un número primo?\n",
    "- ¿Es x mayor a 15?\n",
    "\n",
    "¿Cuál es el número mínimo de preguntas con respuesta si/no que se deben hacer para adivinar el valor de x?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ejemplo:** Meteorólogos del siglo XIX\n",
    "\n",
    "Nos encontramos a finales del siglo XIX. La estación meteorológica de Niebla hace una predicción del tiempo en Valdivia. Esta información se envía a Valdivia a través de telegrafo. Calcule la cantidad de información promedio que envía la estación a Valdivia en cada escenario usando la **entropía de Shannon**\n",
    "\n",
    "- Dos posibilidades: Lluvia y nublado, con probabilidad $1/2$ y $1/2$, respectivamente\n",
    "- Una posibilidad: Lluvia, con probabilidad $1$\n",
    "- Cuatro posibilidades: Lluvia, Nublado, Nubosidad parcial, soleado, con probabilidad $1/2$, $1/4$, $1/8$ y $1/8$, respectivamente\n",
    "\n",
    "**Respuesta:** \n",
    "\n",
    "Las probabilidades de cada mensaje son $2^{-1}$, $2^{-2}$, $2^{-3}$ y $2^{-3}$. Luego la cantidad de información de cada mensaje es: 1, 2, 3 y 3 bits, respectivamente. Por ende la entropía es $1/2 + 1/2 + 3/8 + 3/8 = 1.75$ bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pregunta**\n",
    "\n",
    "Para el **escenario 3** códifique las alternativas usando un alfabeto de códigos binarios\n",
    "\n",
    "> ¿Cómo le asignamos un código a cada alternativa?\n",
    "\n",
    "**Opción 1:** Código de ancho fijo\n",
    "\n",
    "Tenemos cuatro estados.\n",
    "\n",
    "Si todos los estados tienen igual cantidad de bits, necesitamos 2 bits para representarlos: 00, 01, 10, 11\n",
    "\n",
    "En este caso resulta equivalente a asumir equiprobabilidad y la entropía es 2 bits\n",
    "\n",
    "**Opción 2:** Código de ancho variable (prefijo)\n",
    "\n",
    "Se usa 1, 2, 3 y 3 bits para cada estado, según su probabilidad de aparición. En este caso la entropía es 1.75 bits\n",
    "\n",
    "Podemos describir este escenario según\n",
    "\n",
    "- Primera decisión equiprobable: Lluvia **(0)** vs El resto (1)\n",
    "- Segunda decisión equiprobable: Nublado **(10)** vs El resto menos lluvia (11)\n",
    "- Tercera decisión equiprobable: Nubosidad parcial **(110)** vs soleado **(111)**\n",
    "\n",
    "Podemos representar graficamente usando un dendograma como se muestra a continuación\n",
    "\n",
    "<img src=\"../images/dendogram.png\" width=\"500\">\n",
    "\n",
    "- Algoritmo de codificación con forma de árbol en base 2\n",
    "- Los mensajes codificados están en las hojas del árbol\n",
    "- **Código préfijo**: Ningún código puede ser prefijo de otro. \n",
    "- El código prefijo garantiza decodificación sin ambiguedad\n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Decodifique la predicción del tiempo para los próximos tres días: 101100 \n",
    "\n",
    "**Ejercicio** \n",
    "\n",
    "Si el código de lluvia fuera **1** en lugar de 0, decodifique el siguiente mensaje: 11111\n",
    "\n",
    "Este es un ejemplo de código ambiguo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Ejemplo:** Entropía y cantidad de bits del fragmento del famoso texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Código de largo fijo:\", 5*len(texto))\n",
    "# Código de largo variable:\n",
    "freq = np.array(list(Counter(texto).values()))\n",
    "p = freq/np.sum(freq)\n",
    "print(\"Código de largo variable:\", int(-np.sum(p*np.log2(p))*len(texto)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reflexione:\n",
    "\n",
    "- ¿Es la codificación de largo variable *lossless* o *lossy*?\n",
    "- En ciertos casos las palabras son más largas de lo que eran originalmente, ¿Cómo comprimimos entonces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Codificación de Huffman\n",
    "\n",
    "Un algoritmo sencillo de codificación de tipo prefijo:\n",
    "\n",
    "1. Se estima la probabilidad $p_i$ de cada símbolo\n",
    "1. Se ordenan los símbolos en orden descendente según $p_i$\n",
    "1. Juntar los dos con probabilidad menor en un grupo, su probabilidad se suma\n",
    "1. Volver al paso 2 hasta que queden dos grupos\n",
    "1. Asignarle 0 y 1 a las ramas izquierda y derecha del árbol, respectivamente\n",
    "1. El código resultante se lee desde la raiz hasta la hoja\n",
    "\n",
    "<img src=\"../images/huff.png\" width=\"600\">\n",
    "\n",
    "\n",
    "**Debilidad de Huffman:** \n",
    "\n",
    "- Códigos con diccionarios/probabilidades variables\n",
    "- En ese caso combiene usar codificación aritmética o Lempel-Ziv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** Codificación de Huffman del famoso texto\n",
    "\n",
    "Primero estimamos la frecuencia usando `collections.Counter`\n",
    "\n",
    "Luego construimos el dendograma usando `heapq`\n",
    "\n",
    "Terminamos con un diccionario que transforma cada símbolo del texto en un código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemetación adaptada de https://rosettacode.org/wiki/Huffman_coding#Python\n",
    "import heapq\n",
    "# Construir dendograma con las probabilidades ordenadas\n",
    "dendograma = [[frequencia/len(texto), [simbolo, \"\"]] for simbolo, frequencia in Counter(texto).items()]\n",
    "heapq.heapify(dendograma)\n",
    "# Crear el código\n",
    "while len(dendograma) > 1:\n",
    "    lo = heapq.heappop(dendograma)\n",
    "    hi = heapq.heappop(dendograma)\n",
    "    for codigo in lo[1:]:\n",
    "        codigo[1] = '0' + codigo[1]\n",
    "    for codigo in hi[1:]:\n",
    "        codigo[1] = '1' + codigo[1]\n",
    "    heapq.heappush(dendograma, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
    "# Convertir código a diccionario\n",
    "dendograma = sorted(heapq.heappop(dendograma)[1:])\n",
    "dendograma = {simbolo : codigo for simbolo, codigo in dendograma} \n",
    "display(dendograma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos convertir el texto en una tira binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_codificado = \"\"\n",
    "for letra in texto:\n",
    "    texto_codificado += dendograma[letra]\n",
    "\n",
    "display(texto_codificado[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pudemos usar `bytearray` para convertir nuestra tira de caracters 0 y 1 a un arreglo de Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytearray()\n",
    "for i in range(0, len(texto_codificado), 8): # Si el largo del texto no es múltiplo de 8 debemos hacer padding\n",
    "    byte = texto_codificado[i:i+8]\n",
    "    b.append(int(byte, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardando el archivo en disco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../quijote_comprimido.bin\", \"wb\") as f: # Agregamos el diccionario en el header    \n",
    "    f.write(str(dendograma).encode())\n",
    "with open(\"../quijote_comprimido.bin\", \"a\") as f: # salto de linea\n",
    "    f.write(\"\\n\")\n",
    "with open(\"../quijote_comprimido.bin\", \"ab\") as f: # Texto codificado\n",
    "    f.write(bytes(b))\n",
    "!du -B1 --apparent-size ../quijote*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el script [quijote_recuperado.py](./quijote_recuperado.py) puedes revisar como se descomprime el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que las señales pueden tener alta redundancia \n",
    "\n",
    "> Piense por ejemplo en el caso de las imágenes o el lenguaje (contexto)\n",
    "\n",
    "Hemos visto también como comprimir datos explotando esta redudancia\n",
    "\n",
    "En particular \n",
    "\n",
    "1. Transformamos los datos tal de hacerlos \"más independientes\"\n",
    "1. Opcionalmente los cuantizamos para eliminar información menos importante\n",
    "1. Codificamos los datos con una distribución que sea óptima para el canal de transmisión\n",
    "\n",
    "Este último paso es lo que llamamos **codificación de fuente**\n",
    "\n",
    "A continuación revisaremos un importante teorema enunciado por Shannon respecto a la **codificación de un mensaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Teorema de codificación de fuente de Shannon (*Source coding theorem*)\n",
    "\n",
    "\n",
    "Dada una variable aleatoria $X$ con entropía $H(X)$ existe una codificación de largo variable cuyo largo de palabra promedio $\\bar L$ satisface\n",
    "\n",
    "$$\n",
    "H(X) \\leq \\bar L < H(X) + 1\n",
    "$$\n",
    "\n",
    "Es decir que el límite inferior teórico del largo de palabra es $H(X)$. Esta codificación sin pérdida y de largo variable la llamamos **codificación entrópica** \n",
    "\n",
    "Este teorema nos dice cuanto podemos comprimir una señal sin que hayan pérdidas antes de enviarla por un canal (libre de ruido)\n",
    "\n",
    "También justifica la definición de entropía como medida de la cantidad de información\n",
    "\n",
    "\n",
    "Otra forma de ver el teorema:\n",
    "\n",
    "Sea una fuente $X$ que emite $N$ mensajes. \n",
    "\n",
    "> Los N mensajes pueden comprimirse en $N H(X)$ [bits] o más con riesgo de pérdida despreciable ($N\\to\\infty$)\n",
    "\n",
    "Por el contrario\n",
    "\n",
    "> Si comprimimos en menos de $N H(X)$ [bits] la pérdida está garantizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demostración**\n",
    "\n",
    "Sea una codificación C para una variable aleatoria $X$ con N posibles símbolos\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad de ocurrencia $p_i \\in [0, 1]$ con $\\sum_i p_i = 1$ y un largo de código $L_i$\n",
    "\n",
    "Luego el largo promedio de los códigos es\n",
    "\n",
    "$$\n",
    "\\bar L = \\sum_{i=1}^N p_i L_i\n",
    "$$\n",
    "\n",
    "¿Qué valores de $L_i$ resultan en el menor $\\bar L$? \n",
    "\n",
    "> El largo óptimo es $L_i^* = -\\log_2 p_i$ y el promedio sería $\\bar L^* = H(X)$\n",
    "\n",
    "Digamos que proponemos otro largo $\\hat L_i = - \\log_2 q_i$, asumiendo que $\\sum_i q_i = 1$\n",
    "\n",
    "Luego el largo promedio sería\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar L &= \\sum_{i=1}^N p_i \\hat L_i  = - \\sum_{i=1}^N p_i \\log_2 q_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 q_i - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 p_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\nonumber \\\\\n",
    "&= H(X) + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\geq H(X) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Con esto probamos que no hay mejor largo que $-\\log_2 p_i$\n",
    "\n",
    "Notemos que los $L_i^*$ no tendrían porque ser un número enteros \n",
    "\n",
    "```{note}\n",
    "En general la codificación óptima cumple: $H(X) \\leq \\bar L^* < H(X) + 1$\n",
    "```\n",
    "\n",
    "- Se puede estar entre esas cotas con el algoritmo de Huffman\n",
    "- La codificación aritmética en cambio casi siempre llega a la cota inferior\n",
    "- La codificación de Huffman y aritmética son **codificaciones entrópicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de compresión para video\n",
    "\n",
    "En la lección anterior vimos el algoritmo JPEG para compresión de imágenes\n",
    "\n",
    "En esta lección hemos visto la codificación de Huffmann. Con este hemos visto todos los pasos necesarios para implementar el algoritmo JPEG\n",
    "\n",
    "Consideremos ahora el caso en que tenemos un stream de video (sin audio) y queremos transmitirlo comprimido\n",
    "\n",
    "Un algoritmo clásico es Motion-JPEG el cual se basa en el algoritmo JPEG que ya hemos visto. Tiene dos variantes\n",
    "\n",
    "<img src=\"../images/MJPEG.png\"  width=\"500\">\n",
    "\n",
    "- La primera denominada **intraframe** consiste en aplicar el algoritmo JPEG a cada cuadro como si fueran imágenes independientes. Es un algoritmo simple pero ingenuo ya que no está tomando en cuenta que existe mucha correlación entre cuadros\n",
    "- La segunda denominada **interframe** consiste en aplicar el algoritmo JPEG a las diferencias entre cuadros. Si los movimientos son lentos las diferencias entre cuadros serán pequeñas y la codificación entrópica podrá reducir mucho más el tamaño. En general se codifica un cuadro completo denominado keyframe y luego una cierta cantidad de diferencias entre el keyframe y los cuadros siguientes. \n",
    "\n",
    "Esta idea es la que está detrás de algoritmos más modernos como MPEG\n",
    "\n",
    "<img src=\"../images/MPEG.png\" width=\"600\">\n",
    "\n",
    "MPEG es un estándar de codificación para video de tipo inter-frame\n",
    "\n",
    "Explota la redundancia entre los bloques de 8x8 de un cuadro y su sucesor \n",
    "\n",
    "Existen tres tipos de cuador/frame en MPEG-1\n",
    "\n",
    "- I: Se comprime el frame completo con JPEG. Se envía una frame I cada N frames\n",
    "- P: Se predicen las diferencias con respecto al frame I o P anterior. \n",
    "- B: Se calculan diferencias en base al frame I o P más cercano anterior y posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
