{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; }\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from scipy import fftpack\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from style import *\n",
    "\n",
    "def color2bw(img):\n",
    "    return np.dot(img, [0.299, 0.587, 0.114]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ancho de banda de un video\n",
    "\n",
    "- ¿Cúanto pesa una imagen RGB de 1920x1080?\n",
    "\n",
    "<center>1920 1080 3 size(uint8) = 49.766.400 b ~ 50 Mb</center>\n",
    "\n",
    "- Tradicionalmente un video es una secuencia de imágenes a 24 cuadros por segundo (fps)\n",
    "- ¿Cuánto ancho de banda se necesita para ver una película en tiempo real?\n",
    "\n",
    "<center>50Mb  24 fps = 1200 Mb/s = 1.2 Gb/s</center>\n",
    "\n",
    "- Este ancho de banda es infactible ¿Qué necesitamos para que un servicio de streaming pueda funcionar?\n",
    "\n",
    "https://toolstud.io/video/filesize.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compresión de datos\n",
    "\n",
    "- Codificar la información usando \"menos bits\" que la representación original\n",
    "- La compresión puede ser de tipo\n",
    "    - *Lossless* (sin pérdidas): Los datos originales pueden reconstruirse perfectamente\n",
    "    - *Lossy* (con pérdidas): Se reconstruye una versión aproximada de los datos originales\n",
    "\n",
    "\n",
    "Recordemos el modelo de Shannon\n",
    "\n",
    "<img src=\"images/shannon-diagram.svg\" width=\"500\">\n",
    "\n",
    "\n",
    "Podemos hacer un modelo más detallado para el **transmisor** como\n",
    "\n",
    "<img src=\"images/transmitter.svg\" width=\"600\">\n",
    "\n",
    "donde\n",
    "\n",
    "\n",
    "- **Transformación:** Cambia la representación de los datos para eliminar redundancia/correlaciones \n",
    "- **Cuantización:** Escoge un número fijo de valores representativos (diccionario), es decir acorta el \"largo de palabra\" de los datos\n",
    "    - Largo de palabra: Cantidad de bits necesaria para representar un símbolo de código\n",
    "    - Diccionario o alfabeto: Conjunto de símbolos o palabras\n",
    "- **Codificación de fuente:** Convierte el diccionario fijo del cuantizador en un código de largo variable, que es más eficiente de transmitir\n",
    "- **Codificación de canal:** Se encarga de \"robustecer\" el resultado anterior para que sobreviva la transmisión digital (parity, checksum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compresión de imágenes\n",
    "\n",
    "- Una imagen se puede proyectar al espacio de frecuencias sin pérdida de información\n",
    "- En general vimos que la información se concentra en el centro del espectro\n",
    "- Las tres redundancias:\n",
    "    -  **Redundancia interpixel (transformación)** Alta correlación entre píxeles vecinos \n",
    "    - **Redundancia psicovisual (cuantización):** El ojo humano no puede resolver más de 32 niveles de grises:\n",
    "    - **Redundancia de codificación:** Algunos tonos son más comunes que otros \n",
    " \n",
    "\n",
    "¿Cómo explotamos esto para reducir el tamaño de una imagen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Ejemplo: Joint Photographic Experts Group (JPEG)\n",
    "\n",
    "- Formato ampliamente usado para distribuir imágenes digitales\n",
    "- Es un algoritmo de compresión con pérdidas para imágenes (lossy)\n",
    "- Explota las siguientes características del sistema visual humano\n",
    "    - Somos más sensibles a la iluminación que al color\n",
    "    - Somos menos sensibles a los componentes de alta frecuencia\n",
    "- Más componentes descartados: mayor compresión, y peor la calidad\n",
    "\n",
    "\n",
    "Algoritmo:\n",
    "1. Paso preliminar: RGB a YCbCr (y downsampling 4:2:2)\n",
    "1. Transformación con **Discrete Cosine Transform** (DCT) en bloques de 8x8 sin traslape\n",
    "1. Cuantización escalar\n",
    "1. Codificación de Huffman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Espacio YCbCr\n",
    "- Y es la luminancia\n",
    "- Cb y Cr son los componentes cromáticos azul y rojo, respectivamente\n",
    "\n",
    "$$\n",
    "Y = K_R R + K_G G + K_B B \\\\\n",
    "C_b = 0.5 \\frac{B - Y}{1 - K_B} \\\\\n",
    "C_R = 0.5 \\frac{R - Y}{1 - K_R}\n",
    "$$\n",
    "donde $K_R + K_G + K_B = 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ITU-R BT.601 \n",
    "dogo_color = plt.imread('images/lobo.jpg')\n",
    "Y = color2bw(dogo_color)\n",
    "\n",
    "plt.matshow(Y, cmap=plt.cm.Greys_r); plt.axis('off'); plt.title('Y')\n",
    "C_b = np.dot(dogo_color, [-0.168736, -0.3312, 0.5])\n",
    "plt.matshow(C_b, cmap=plt.cm.Greys_r); plt.axis('off'); plt.title('Cb')\n",
    "C_r = np.dot(dogo_color, [0.5, -0.4186, -0.0813])\n",
    "plt.matshow(C_r, cmap=plt.cm.Greys_r); plt.axis('off'); plt.title('Cr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close(); fig, ax = plt.subplots(figsize=(9, 5), tight_layout=True)\n",
    "\n",
    "def update(q_Y, q_C):\n",
    "    ax.cla(); ax.axis('off')\n",
    "    Y = np.round(np.dot(dogo_color, [0.299, 0.587, 0.114])*q_Y/255.0)*255.0/q_Y\n",
    "    C_b = 128+np.round(np.dot(dogo_color, [-0.168736, -0.3312, 0.5])*q_C/255.0)*255.0/q_C\n",
    "    C_r = 128+np.round(np.dot(dogo_color, [0.5, -0.4186, -0.0813])*q_C/255.0)*255.0/q_C\n",
    "    rgb = np.zeros(shape=(Y.shape[0], Y.shape[1], 3))\n",
    "    rgb[:,:,0] = Y + 1.402 * (C_r-128)\n",
    "    rgb[:,:,1] = Y - .34414 * (C_b-128) - .71414 * (C_r-128)\n",
    "    rgb[:,:,2] = Y + 1.772 * (C_b-128)\n",
    "    np.putmask(rgb, rgb > 255, 255);\n",
    "    np.putmask(rgb, rgb < 0, 0);    \n",
    "    ax.imshow(np.uint8(rgb));\n",
    "   \n",
    "interact(update, q_Y=SelectionSlider_nice(options=[1, 2, 4, 8, 16, 32, 64, 128], value=128, description=\"Niveles de Y\"),\n",
    "         q_C=SelectionSlider_nice(options=[1, 2, 4, 8, 16, 32, 64, 128], value=128, description=\"Niveles de C\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete Cosine Transform (DCT)\n",
    "\n",
    "Sea una señal discreta y bidimensional $g[n_1, n_2]$ con índices $n_1 \\in [0, N_1-1]$ y $n_2 \\in [0, N_2-1]$ su DCT es \n",
    "\n",
    "$$\n",
    "G_C[k_1, k_2] = \\sum_{n_1=0}^{N_1-1} \\sum_{n_2=0}^{N_2-1} 4 g[n_1, n_2] \\cos \\left ( \\frac{\\pi k_1}{2N_1}(2n_1+1)  \\right) \\cos \\left ( \\frac{\\pi k_2}{2N_2}(2n_2+1)  \\right) \n",
    "$$\n",
    "y su inversa\n",
    "\n",
    "$$\n",
    "g[n_1, n_2] = \\frac{1}{N_1 N_2}\\sum_{k_1=0}^{N_1-1} \\sum_{k_2=0}^{N_2-1} w[k_1]w[k_2]G[k_1, k_2] \\cos \\left ( \\frac{\\pi k_1}{2N_1}(2n_1+1)  \\right) \\cos \\left ( \\frac{\\pi k_2}{2N_2}(2n_2+1)  \\right), \n",
    "$$\n",
    "\n",
    "donde \n",
    "$$\n",
    "w[k] =\\begin{cases}\n",
    "1/2 & \\text{ssi} ~~ k=0\\\\\n",
    "1 & \\text{ssi} ~~ k \\neq 0\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "La DCT bidimensional:\n",
    "- es lineal, y cumple el principio de conservación de energía\n",
    "- se puede descomponer en 2 aplicaciones de la DCT 1D\n",
    "- es equivalente a la DFT de una señal \"simetricamente extendida\":\n",
    "\n",
    "$$\n",
    "y[k] =\\begin{cases}\n",
    "x[k] & \\text{ssi} ~~ k<N\\\\\n",
    "x[2N-1-k] & \\text{ssi} ~~ N \\leq k < 2N - 1 \n",
    "\\end{cases} \n",
    "$$\n",
    "- Es decir que podemos usar el algoritmo FFT para calcular eficientemente la DCT\n",
    "\n",
    "**Ojo:** La convolución en espacio original no es multiplicación en el espacio DCT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La base de la DCT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 32, step=1)\n",
    "X, Y = np.meshgrid(x, x)\n",
    "fig, ax = plt.subplots(8, 8, figsize=(9, 8), tight_layout=False)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\n",
    "for n in range(ax.shape[0]):\n",
    "    for m in range(ax.shape[1]):\n",
    "        cos_x = np.cos(np.pi*(2*X+1)*m/(2*len(x)))\n",
    "        cos_y = np.cos(np.pi*(2*Y+1)*n/(2*len(x)))\n",
    "        ax[n, m].matshow(cos_x*cos_y, cmap=plt.cm.RdBu_r, vmin=-1, vmax=1)\n",
    "        ax[n, m].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo: Transformación de una imagen usando DCT en bloques de 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "img_seadoge = color2bw(plt.imread('images/lobo.jpg'))  \n",
    "fig, ax = plt.subplots(figsize=(9, 6)); ax.axis('off')\n",
    "ax.imshow(img_seadoge, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "DCT2 = lambda g, norm='ortho': fftpack.dct( fftpack.dct(g, axis=0, norm=norm), axis=1, norm=norm)\n",
    "IDCT2 = lambda G, norm='ortho': fftpack.idct( fftpack.idct(G, axis=0, norm=norm), axis=1, norm=norm)\n",
    "\n",
    "imsize = img_seadoge.shape\n",
    "dct_matrix = np.zeros(shape=imsize)\n",
    "dft_matrix = np.zeros(shape=imsize, dtype=np.complex128)\n",
    "\n",
    "for i in range(0, imsize[0], 8):\n",
    "    for j in range(0, imsize[1], 8):\n",
    "        dct_matrix[i:(i+8),j:(j+8)] = DCT2( img_seadoge[i:(i+8),j:(j+8)] )\n",
    "        dft_matrix[i:(i+8),j:(j+8)] = fftpack.fft2( img_seadoge[i:(i+8),j:(j+8)] )  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "¿Cómo se ven uno a uno la DCT de los bloques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "def plot_values(ax, tile, fontsize=16):\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            label = tile[i, j]\n",
    "            ax.text(i, j, int(label), fontsize=fontsize, \n",
    "                    color='black', ha='center', va='center')\n",
    "            \n",
    "def update(block_idx=1):\n",
    "    for ax_ in ax:\n",
    "        ax_.cla(); ax_.axis('off')\n",
    "    tile = img_seadoge[8*block_idx:8*block_idx+8, 8*block_idx:8*block_idx+8]\n",
    "    ax[0].imshow(tile, cmap=plt.cm.Greys_r, \n",
    "                 vmin=img_seadoge.min(), vmax=img_seadoge.max())\n",
    "    ax[0].set_title(\"%d 8x8 image block\" %(block_idx)); \n",
    "    plot_values(ax[0], tile)\n",
    "    tile = dct_matrix[8*block_idx:8*block_idx+8, 8*block_idx:8*block_idx+8]\n",
    "    ax[1].imshow(tile, cmap=plt.cm.Greys_r, \n",
    "                 vmin=dct_matrix.min(), vmax=dct_matrix.max())\n",
    "    ax[1].set_title(\"8x8 DCT\")\n",
    "    plot_values(ax[1], tile)\n",
    "\n",
    "interact(update, block_idx=IntSlider_nice(min=0, max=100, value=0, \n",
    "                                          description=\"Bloque\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "¿Cómo se ven los bloques de la DCT si los concatenamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.close(); \n",
    "fig, ax = plt.subplots(figsize=(10, 7), tight_layout=True)\n",
    "ax.axis('off')\n",
    "ax.matshow(dct_matrix, cmap=plt.cm.Greys_r, vmin=0, vmax=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Comparemos ahora la DFT y DCT en términos de reconstrucción con pérdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 6), tight_layout=False)\n",
    "fig.subplots_adjust(left=0.01, right=0.99, top=0.94, bottom=0.01)\n",
    "def update(percent):\n",
    "    for ax_ in ax.ravel():\n",
    "        ax_.cla(); ax_.axis('off')\n",
    "    imsize = img_seadoge.shape\n",
    "    Npixels = imsize[0]*imsize[1]\n",
    "    im_dct, im_dft = np.zeros(imsize), np.zeros(imsize)\n",
    "    I = np.unravel_index(np.argsort(np.absolute(dct_matrix), axis=None)[::-1], \n",
    "                         dct_matrix.shape)\n",
    "    dct_thresh = dct_matrix.copy()\n",
    "    dct_thresh[I[0][int(Npixels*percent/100):], \n",
    "               I[1][int(Npixels*percent/100):]] = 0\n",
    "    I = np.unravel_index(np.argsort(np.absolute(dft_matrix), axis=None)[::-1], \n",
    "                         dft_matrix.shape)\n",
    "    dft_thresh = dft_matrix.copy()\n",
    "    dft_thresh[I[0][int(Npixels*percent/100):], \n",
    "               I[1][int(Npixels*percent/100):]] = 0\n",
    "    for i in range(0, imsize[0], 8):\n",
    "        for j in range(0, imsize[1], 8):\n",
    "            im_dct[i:(i+8),j:(j+8)] = IDCT2( dct_thresh[i:(i+8),j:(j+8)] )\n",
    "            im_dft[i:(i+8),j:(j+8)] = np.real(fftpack.ifft2( dft_thresh[i:(i+8),j:(j+8)] ))\n",
    "    \n",
    "    ax[0, 0].imshow(im_dft, cmap=plt.cm.Greys_r); \n",
    "    ax[0, 0].set_title(\"DFT\")\n",
    "    ax[1, 0].imshow(im_dft[200:400, 250:600], cmap=plt.cm.Greys_r); \n",
    "    ax[0, 1].imshow(im_dct, cmap=plt.cm.Greys_r); \n",
    "    ax[0, 1].set_title(\"DCT\")\n",
    "    ax[1, 1].imshow(im_dct[200:400, 250:600], cmap=plt.cm.Greys_r); \n",
    "    \n",
    "interact(update, percent=SelectionSlider_nice(options=[1., 2.5, 5., 7.5, 10., 20], value=20,\n",
    "                                          description=\"Porcentaje de componentes retenidos\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La DCT es equivalente a la DFT de la imagen simetricamente extendida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "img_seadogo_sym = np.hstack((img_seadoge, np.fliplr(img_seadoge)))\n",
    "img_seadogo_sym = np.vstack((np.flipud(img_seadogo_sym), img_seadogo_sym))\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "ax.imshow(img_seadogo_sym, cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- DCT y efectos de borde\n",
    "- optimalidad en termines de aproximacion a transformacion KL\n",
    "- optimalidad en terminos de señal markoviana con correlacion positiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cuantización escalar\n",
    "\n",
    "- Objetivo: Atacar la redundancia psicovisual\n",
    "- Es una operación de redondeo/truncamiento\n",
    "- Se define por: \n",
    "    - número de niveles $L$, \n",
    "    - fronteras de decisión $d_i$ \n",
    "    - valor de las representaciones $r_i$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    Q(x)=\n",
    "    \\begin{cases}\n",
    "      r_1, & d_0 < x \\leq d_1 \\\\\n",
    "      \\vdots & \\vdots \\\\\n",
    "      r_i, & d_{i-1} < x \\leq d_i \\\\\n",
    "      \\vdots & \\vdots \\\\\n",
    "      r_L, & d_{L-1} < x \\leq d_L\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Ejemplo: Cuantización uniforme\n",
    "\n",
    "Sea un rango de valores en $[-V, V]$, una cuantización uniforme de $L$ niveles sería\n",
    "\n",
    "$$\n",
    "d_0 = -V, d_{L} = V\n",
    "$$\n",
    "La separación entre niveles es fija\n",
    "$$\n",
    "d_{i} = d_{i-1} + \\Delta  = d_0 + i \\Delta = - V + i \\frac{2V}{L}\n",
    "$$\n",
    "y el valor de representación es el punto medio de cada nivel\n",
    "$$\n",
    "r_i = \\frac{1}{2} (d_i + d_{i-1}) = -V + (2i-1) \\frac{2V}{L}\n",
    "$$ \n",
    " \n",
    "\n",
    "\n",
    "### Error de cuantización\n",
    "\n",
    "Es la distancia entre el valor real y su versión cuantizada\n",
    "$$\n",
    "d_c = \\|Q(x) -x\\| \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cuantización vectorial\n",
    "\n",
    "Es la extensión del caso anterior a datos multidimensionales, por ejemplo píxeles en formato RGB\n",
    "\n",
    "- Sea un conjunto de datos N-dimensionales $\\{x_1, x_2, \\ldots, x_N\\}$ con $x_i \\in \\mathbb{R}^D$\n",
    "- Se busca un conjunto reducido de prototipos $\\{r_1, r_2, \\ldots, r_L\\}$ donde $L < N$\n",
    "- La función de cuantización es $Q(x) = r_k$ donde $k = \\text{arg}\\min_i \\|x - r_i\\|$\n",
    "- Hay que definir una métrica de distancia (*e.g* norma L2)\n",
    "\n",
    "Los prototipos pueden\n",
    "- ser fijos y estar guardados (look-up table)\n",
    "- construirse en base a una regla simple (cubos uniformes)\n",
    "- seleccionar adaptivamente usando el\n",
    "\n",
    "### Algoritmo *Learning Vector Quantization* (LVQ)\n",
    "1. Definir una función de distancia\n",
    "1. Inicializar con prototipos aleatorios\n",
    "1. Asignar los datos a su prototipo más cercano \n",
    "1. Actualizar los prototipos como el valor medio de sus datos asignados\n",
    "1. Volver al paso 3 hasta converger\n",
    "\n",
    "Si se usa norma euclidiana (L2) \n",
    "$$\n",
    "\\|x - r_i \\|_2 = \\sqrt{ \\sum_{d=1}^D (x[d] - r_i[d])^2 }\n",
    "$$\n",
    "se obtiene el famoso algoritmo *K-means*\n",
    "\n",
    "<center><img src=\"images/kmeans.gif\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import vq\n",
    "from sklearn.cluster import KMeans\n",
    "img_color = plt.imread(\"images/atardecer.jpg\")\n",
    "img_pixels = np.reshape(img_color, (-1, 3))\n",
    "kmeans = KMeans(n_clusters=10).fit(np.float32(img_pixels)) \n",
    "centroids = kmeans.cluster_centers_\n",
    "qnt,_ = vq(np.float32(img_pixels), centroids)\n",
    "centers_idx = np.reshape(qnt, (img_color.shape[0], img_color.shape[1]))\n",
    "clustered = np.uint8(centroids[centers_idx])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4), tight_layout=False)\n",
    "fig.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "ax[0].imshow(img_color); ax[0].axis('off');\n",
    "ax[1].imshow(clustered); ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=((7, 5)))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], \n",
    "           c=centroids/255., s=500);\n",
    "ax.scatter(img_pixels[:, 0], img_pixels[:, 1], img_pixels[:, 2], \n",
    "           c=img_pixels/255., s=1, alpha=0.01);\n",
    "ax.set_xlim([0, 255]); ax.set_ylim([0, 255]); ax.set_zlim([0, 255]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cuantización en JPEG\n",
    "\n",
    "- JPEG cuantiza en el espacio de frecuencia\n",
    "- Se cuantizan los bloques de 8x8 componentes DCT \n",
    "- El nivel de los componentes se redondea según una matriz de cuantización Q\n",
    "- Q fue diseñada tal que componentes de alta frecuencia se cuantizan en menos niveles\n",
    "- El bloque cuantizado se obtiene como $\\text{ROUND}\\left(\\frac{G_C}{Q}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Q = np.array([[16, 11, 10, 16, 24, 40, 51, 61],[12, 12, 14, 19, 26, 58, 60, 55],\n",
    "              [14, 13, 16, 24, 40, 57, 69, 56],[14, 17, 22, 29, 51, 87, 80, 62],\n",
    "              [18, 22, 37, 56, 68, 109, 103, 77],[24, 35, 55, 64, 81, 104, 113, 92],\n",
    "              [49, 64, 78, 87, 103, 121, 120, 101],[72, 92, 95, 98, 112, 100, 103, 99]])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 4), tight_layout=True)\n",
    "def update(block_idx=1):\n",
    "    for ax_ in ax:\n",
    "        ax_.cla(); ax_.axis('off')\n",
    "    tile = img_seadoge[8*block_idx:8*block_idx+8, 8*block_idx:8*block_idx+8]\n",
    "    ax[0].imshow(tile, cmap=plt.cm.Greys_r)\n",
    "    ax[0].set_title(\"8x8 %d'th block\" %(block_idx)); \n",
    "    plot_values(ax[0], tile, fontsize=12)\n",
    "    tile = dct_matrix[8*block_idx:8*block_idx+8, 8*block_idx:8*block_idx+8]\n",
    "    ax[1].imshow(tile, cmap=plt.cm.Greys_r)\n",
    "    ax[1].set_title(\"8x8 DCT\\n %d nonzero\" %(np.count_nonzero(tile))); \n",
    "    plot_values(ax[1], tile, fontsize=12)\n",
    "    quant = np.round(tile/Q)\n",
    "    ax[2].imshow(quant, cmap=plt.cm.Greys_r)\n",
    "    ax[2].set_title(\"8x8 Quantized\\n%d nonzero\" %(np.count_nonzero(quant))); \n",
    "    plot_values(ax[2], quant, fontsize=12)\n",
    "\n",
    "interact(update, block_idx=IntSlider_nice(min=0, max=100, value=0, description=\"Block tile\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "area = img_seadoge.shape[0]*img_seadoge.shape[1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), tight_layout=False)\n",
    "fig.subplots_adjust(left=0.01, right=0.99, top=0.94, bottom=0.01)\n",
    "\n",
    "def update(percent):\n",
    "    for ax_ in ax.ravel():\n",
    "        ax_.cla(); ax_.axis('off')\n",
    "    im_dct = np.zeros(imsize)\n",
    "    nnz = np.zeros(dct_matrix.shape)\n",
    "    if (percent < 50):\n",
    "        S = 5000/percent\n",
    "    else:\n",
    "        S = 200 - 2*percent \n",
    "    Q_dyn = np.floor((S*Q + 50) / 100);\n",
    "    Q_dyn[Q_dyn == 0] = 1\n",
    "    for i in range(0, imsize[0], 8):\n",
    "        for j in range(0, imsize[1], 8):\n",
    "            quant = np.round(dct_matrix[i:(i+8),j:(j+8)]/Q_dyn) \n",
    "            im_dct[i:(i+8),j:(j+8)] = IDCT2(quant)\n",
    "            nnz[i, j] = np.count_nonzero(quant)\n",
    "    \n",
    "    ax[0].imshow(img_seadoge, cmap=plt.cm.Greys_r); \n",
    "    ax[0].set_title(\"%0.2f MB\" %(area*8/1e+6))\n",
    "    ax[1].imshow(im_dct, cmap=plt.cm.Greys_r); \n",
    "    ax[1].set_title(\"%0.2f MB\" %(np.sum(nnz)*8/1e+6))\n",
    "interact(update, percent=FloatSlider_nice(min=0, max=100, step=0.01, value=100, \n",
    "                                     description=\"Nivel de compresión\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Codificación de fuente\n",
    "\n",
    "- Es el proceso que asigna un código a cada mensaje\n",
    "- El largo de las \"palabras de código\" puede ser **fijo** o **variable**\n",
    "- **Ejemplo:** Queremos codificar las letras del alfabeto (27) usando dos símbolos (código binario)\n",
    "    - Para largo fijo necesitamos al menos 5 bits ($2^5 = 32$)\n",
    "- Pero algunas letras se ocupan más que otras (*e.g.* vocales) \n",
    "- ¿Podemos aprovechar esto para comprimir un mensaje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "with open('quijote.txt', 'r') as file:\n",
    "    texto = file.read()\n",
    "print(texto)\n",
    "texto = texto.translate({ord(k): None for k in string.digits})\n",
    "for symbol in string.punctuation:\n",
    "    texto = texto.replace(symbol, \"\")\n",
    "texto = texto.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "texto = texto.lower().encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(\"\\nLargo del texto: {0}, Cantidad de bits: {1}\".format(len(texto), len(texto)*5))\n",
    "Counter(texto).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Intuición:** Podríamos reducir la cantidad de bits si usamos códigos más cortos para las letras más frecuentes\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teoría de la información\n",
    "\n",
    "- Estudio matemático sobre la cuantificación y transmisión de la información \n",
    "- Propuesto por **Claude Shannon** en 1948: *A Mathematical Theory of Communication*\n",
    "- Proporciona medidas para describir la información de un proceso: **Entropía** e **Información Mutua**\n",
    "- Aplicaciones en telecomunicaciones, computación y biología (genética)\n",
    "- Fuerte influencia en la teoría de codificación y compresión\n",
    "\n",
    "### Las dos fuentes\n",
    "\n",
    "Sean dos fuentes **F1** y **F2** que pueden emitir uno entre cuatro símbolos: $A$, $B$, $C$ o $D$\n",
    "\n",
    "**F1** es completamente aleatoria, es decir: $P(A) = P(B) = P(C) = P(D) = \\frac{1}{4}$\n",
    "\n",
    "Si queremos predecir el próximo valor emitido por **F1** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer?\n",
    "\n",
    "\n",
    "<img src=\"images/information1.svg\" width=\"600\">\n",
    "\n",
    "> La respuesta es 2 para cualquiera de los símbolos\n",
    "\n",
    "**F2** en cambio emite $A$, $B$, $C$ y $D$ con probabilidades $P(A) =\\frac{1}{2}$, $P(B) =\\frac{1}{4}$, $P(C) = \\frac{1}{8}$ y $P(D) =\\frac{1}{8}$, respectivamente\n",
    "\n",
    "Si queremos predecir el próximo valor retornado por **F2** ¿Cúal es el número mínimo de preguntas con respuesta si/no que debemos hacer? \n",
    "\n",
    "<img src=\"images/information2.svg\" width=\"800\">\n",
    "\n",
    "> La respuesta es 1 para $A$, 2 para $B$ y 3 para $C$ y $D$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cantidad de información (según Shannon)\n",
    "\n",
    "La cantidad de información de un símbolo $x$ es el logaritmo en base dos del recíproco de su probabilidad de aparición\n",
    "\n",
    "$$\n",
    "I(x) = \\log_2 \\left(\\frac{1}{P(x)} \\right) = \\log_2 P(x)^{-1} = - \\log_2 P(x),\n",
    "$$\n",
    "\n",
    "que es equivalente a la mínima cantidad de preguntas si/no que debemos hacer para adivinar su valor\n",
    "\n",
    "La cantidad de información se mide en **bits**\n",
    "\n",
    ">Un **bit** es la cantidad de información que se requiere para escoger entre **dos** alternativas equiprobables\n",
    "\n",
    "La cantidad de información es también llamada **sorpresa**\n",
    "\n",
    "> Mientras más improbable es un símbolo, más nos sorprendemos cuando observamos que ocurre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropía\n",
    "\n",
    "Sea una variable aleatoria $X$ (fuente) con $N$ resultados posibles (símbolos) $\\{x_1, x_2, \\ldots, x_N\\}$\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad $p_i \\in [0, 1]$ y $\\sum_{i=1}^N p_i = 1$ \n",
    "\n",
    "Cada símbolo tiene una cantidad de información  $I(x_i) = -\\log_2 \\left( p_i \\right)$ \n",
    "\n",
    "Definimos la **cantidad de información promedio** de $X$ como\n",
    "$$\n",
    "\\begin{align}\n",
    "H (X) &= \\mathbb{E}_{x\\sim X} \\left [ - \\log P(X=x) \\right ]  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N P(X=x_i) \\log_2 P(X=x_i)  \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i  \\quad \\text{[bits/símbolo]} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "que se conoce como **Entropía de Shannon** \n",
    "\n",
    "### Propiedades\n",
    "- La entropía es siempre positiva $H(X) \\geq 0$. La igualdad se cumple si un $x_i$ tiene $p_i=1$ (caso más predecible)\n",
    "- La entropia está acotada $H(X) \\leq H_0$, donde $H_0= \\log_2(N)$ es la entropia si $p_i = \\frac{1}{N}~ \\forall i$ (caso menos predecible)\n",
    "- La redundancia de $X$ es $1 - H(X)/H_0$\n",
    "\n",
    "> Mientras más predecible es $X$ menor es su entropía y mayor es su redundancia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### El retorno de las dos fuentes\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F1**?\n",
    "\n",
    "> $1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} + 1 \\frac{1}{4} = 2$ preguntas por símbolo. Su entropía es $2$ [bits]\n",
    "\n",
    "En promedio, ¿Cuántas preguntas por símbolo hace la fuente **F2**?\n",
    "\n",
    "> $1 \\frac{1}{2} + 2 \\frac{1}{4} + 3 \\frac{1}{8} + 3 \\frac{1}{8} = 1.75$ preguntas por símbolo. Su entropía es $1.75$ [bits]\n",
    "\n",
    "Si cada fuente retorna un mensaje de 100 símbolos ¿Cúanta información produjo cada una?\n",
    "\n",
    "> **F1** produce 200 bits mientras que **F2** produce 175 bits\n",
    "\n",
    "Mientras más predecible menos información se necesita\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: Moneda con truco\n",
    "\n",
    "- Sea una variable aleatoria $X$ que modela el resultado de lanzar una moneda\n",
    "- Asumamos que el resultado puede tomar solo dos valores: Cara $o$ o Cruz $x$\n",
    "- La probabilidad de que salga cara es $p_o = p$\n",
    "- La probabilidad de que salga cruz es $p_x = 1- p$\n",
    "- Luego la entropía es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X) &= -\\sum_{i=1}^2 p_i \\log_2 p_i \\nonumber \\\\ \n",
    "&= -p_x \\log (p_x) - p_o \\log p_o \\nonumber \\\\\n",
    "&= - p \\log(p) - (1-p) \\log(1-p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reflexione:\n",
    "- ¿En que casos la entropía es mínima? ¿En qué caso es máxima?\n",
    "- ¿Puedes relacionar la entropía con la aleatoridad/sorpresa del resultado de lanzar la moneda?\n",
    "\n",
    "\n",
    "Ojo: $\\lim_{z\\to 0^+} z \\log 1/z = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = np.linspace(0.01, 0.99, num=100)\n",
    "H = -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "fig, ax = plt.subplots(1, figsize=(6, 4), sharex=True)\n",
    "ax.set_xlabel('p')\n",
    "ax.plot(p, -np.log2(p), label='I(o)', lw=3)\n",
    "ax.plot(p, -np.log2(1-p), label='I(x)', lw=3)\n",
    "ax.plot(p, H, label='H(X)', lw=3)\n",
    "ax.set_ylim([0, 3])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:**\n",
    "\n",
    "Sea una fuente que escupe un entero x que está entre 0 y 31\n",
    "\n",
    "Considere el resultado de las siguientes preguntas ¿Cúal tiene mayor entropía?\n",
    "- ¿Es x igual a 0?\n",
    "- ¿Es x un número primo?\n",
    "- ¿Es x mayor a 15?\n",
    "\n",
    "¿Cuál es el número mínimo de preguntas con respuesta si/no que se deben hacer para adivinar el valor de x?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: Meteorólogos del siglo XIX\n",
    "\n",
    "- Nos encontramos a finales del siglo XIX\n",
    "- La estación meteorológica de Niebla hace una predicción del tiempo en Valdivia\n",
    "- Esta información se envía a Valdivia a través de telegrafo\n",
    "- Calcule la cantidad de información promedio que envía la estación a Valdivia en cada escenario usando la **entropía de Shannon**\n",
    "\n",
    "**Escenario 1:** Dos posibilidades: Lluvia y nublado, con probabilidad $1/2$ y $1/2$, respectivamente\n",
    "\n",
    "**Escenario 2:** Una posibilidad: Lluvia, con probabilidad $1$\n",
    "\n",
    "**Escenario 3:** Cuatro posibilidades: Lluvia, Nublado, Nubosidad parcial, soleado, con probabilidad $1/2$, $1/4$, $1/8$ y $1/8$, respectivamente\n",
    "\n",
    "1. Las probabilidades de cada mensaje son $2^{-1}$, $2^{-2}$, $2^{-3}$ y $2^{-3}$\n",
    "1. La cantidad de información de cada mensaje es: 1, 2, 3 y 3 bits, respectivamente\n",
    "1. La entropía es $1/2 + 1/2 + 3/8 + 3/8 = 1.75$ bits\n",
    "\n",
    "Para el **escenario 3** códifique las alternativas usando un alfabeto de códigos binarios\n",
    "\n",
    "> ¿Cómo le asignamos un código a cada alternativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Código de ancho fijo\n",
    "\n",
    "- Tenemos cuatro estados, necesitamos 2 bits\n",
    "- Cada estado: 00, 01, 10, 11\n",
    "- En este caso resulta equivalente a asumir equiprobabilidad \n",
    "- La entropía es 2 bits\n",
    "\n",
    "### Código de ancho variable (prefijo)\n",
    "\n",
    "- Se usa 1, 2, 3 y 3 bits para cada estado, según su probabilidad de aparición\n",
    "- La entropía es 1.75 bits\n",
    "- Podemos describir este escenario según\n",
    "    - Primera decisión equiprobable: Lluvia **(0)** vs El resto (1)\n",
    "    - Segunda decisión equiprobable: Nublado **(10)** vs El resto menos lluvia (11)\n",
    "    - Tercera decisión equiprobable: Nubosidad parcial **(110)** vs soleado **(111)**\n",
    "- Podemos escribir esto como un dendograma\n",
    "\n",
    "<center><img src=\"images/dendogram.png\" width=\"600\"></center>\n",
    "\n",
    "- Algoritmo de codificación con forma de árbol en base 2\n",
    "- Los mensajes codificados están en las hojas del árbol\n",
    "- **Código préfijo**: Ningún código puede ser prefijo de otro. \n",
    "- El código prefijo garantiza decodificación sin ambiguedad\n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Decodifique la predicción del tiempo para los próximos tres días: 101100 \n",
    "\n",
    "**Ejemplo de código ambiguo** \n",
    "\n",
    "Si el código de lluvia fuera **1** en lugar de 0, decodifique el siguiente mensaje: 11111\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo: Entropía y cantidad de bits del fragmento del famoso texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Código de largo fijo:\n",
    "print(5*len(texto))\n",
    "# Código de largo variable:\n",
    "freq = np.array(list(Counter(texto).values()))\n",
    "p = freq/np.sum(freq)\n",
    "print(int(-np.sum(p*np.log2(p))*len(texto)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Reflexionemos:** \n",
    "- ¿Es la codificación de largo variable *lossless* o *lossy*?\n",
    "- En ciertos casos las palabras son más largas de lo que eran originalmente, ¿Cómo comprimimos entonces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Codificación de Huffman\n",
    "\n",
    "Un algoritmo sencillo de codificación de tipo prefijo:\n",
    "\n",
    "1. Se estima la probabilidad $p_i$ de cada símbolo\n",
    "1. Se ordenan los símbolos en orden descendente según $p_i$\n",
    "1. Juntar los dos con probabilidad menor en un grupo, su probabilidad se suma\n",
    "1. Volver al paso 2 hasta que queden dos grupos\n",
    "1. Asignarle 0 y 1 a las ramas izquierda y derecha del árbol, respectivamente\n",
    "1. El código resultante se lee desde la raiz hasta la hoja\n",
    "\n",
    "<center><a href=\"http://www.skylondaworks.com/sc_huff.htm\"><img src=\"images/huff.gif\" width=\"600\"></a></center>\n",
    "\n",
    "\n",
    "**Debilidad de Huffman:** \n",
    "- Códigos con diccionarios/probabilidades variables\n",
    "- En ese caso combiene usar codificación aritmética o Lempel-Ziv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que en general las señales/datos tienen alta redundancia \n",
    "\n",
    "> Piense por ejemplo en el caso de las imágenes o el lenguaje (contexto)\n",
    "\n",
    "Hemos visto también como comprimir datos\n",
    "\n",
    "En particular \n",
    "1. Transformamos los datos tal de hacerlos \"más independientes\" y opcionalmente los cuantizamos\n",
    "1. Codificamos los datos con una distribución que sea óptima para el canal de transmisión\n",
    "\n",
    "Este último paso es lo que llamamos **codificación de fuente**\n",
    "\n",
    "A continuación revisaremos un importante teorema enunciado por Shannon respecto a la **codificación de un mensaje**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Teorema de codificación de fuente de Shannon (*Source coding theorem*)\n",
    "\n",
    "\n",
    ">Dada una variable aleatoria $X$ con entropía $H(X)$ existe una codificación de largo variable cuyo largo de palabra promedio $\\bar L$ satisface\n",
    "\n",
    ">$$\n",
    "H(X) \\leq \\bar L < H(X) + 1\n",
    "$$\n",
    "\n",
    "Es decir que el límite inferior teórico del largo de palabra es $H(X)$\n",
    "\n",
    "Esta codificación sin pérdida y de largo variable la llamamos **codificación entrópica** \n",
    "\n",
    "Este teorema \n",
    "- nos dice cuanto podemos comprimir una señal sin que hayan pérdidas antes de enviarla por un canal (libre de ruido)\n",
    "- justifica la definición de entropía como medida de la cantidad de información\n",
    "\n",
    "\n",
    "Otra forma de ver el teorema\n",
    "\n",
    "Sea una fuente $X$ que emite $N$ mensajes. \n",
    "\n",
    "> Los N mensajes pueden comprimirse en $N H(X)$ [bits] o más con riesgo de pérdida despreciable ($N\\to\\infty$)\n",
    "\n",
    "Por el contrario\n",
    "\n",
    "> Si comprimimos en menos de $N H(X)$ [bits] la pérdida está garantizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probando el teorema\n",
    "\n",
    "Sea una codificación C para una variable aleatoria $X$ con N posibles símbolos\n",
    "\n",
    "Cada símbolo $x_i$ tiene una probabilidad de ocurrencia $p_i \\in [0, 1]$ con $\\sum_i p_i = 1$ y un largo de código $L_i$\n",
    "\n",
    "Luego el largo promedio de los códigos es\n",
    "\n",
    "$$\n",
    "\\bar L = \\sum_{i=1}^N p_i L_i\n",
    "$$\n",
    "\n",
    "¿Qué valores de $L_i$ resultan en el menor $\\bar L$? \n",
    "\n",
    "> El largo óptimo es $L_i^* = -\\log_2 p_i$ y el promedio sería $\\bar L^* = H(X)$\n",
    "\n",
    "Digamos que proponemos otro largo $\\hat L_i = - \\log_2 q_i$, asumiendo que $\\sum_i q_i = 1$\n",
    "\n",
    "Luego el largo promedio sería\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar L &= \\sum_{i=1}^N p_i \\hat L_i  = - \\sum_{i=1}^N p_i \\log_2 q_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 q_i - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 p_i \\nonumber \\\\\n",
    "&= - \\sum_{i=1}^N p_i \\log_2 p_i + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\nonumber \\\\\n",
    "&= H(X) + \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\geq H(X) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "> Con esto probamos que no hay mejor largo que $-\\log_2 p_i$\n",
    "\n",
    "Notemos que los $L_i^*$ no tendrían porque ser un número enteros \n",
    "\n",
    "> En general la codificación óptima cumple: $H(X) \\leq \\bar L^* < H(X) + 1$\n",
    "\n",
    "- Se puede estar entre esas cotas con el algoritmo de Huffman\n",
    "- La codificación aritmética en cambio casi siempre llega a la cota inferior\n",
    "- La codificación de Huffman y aritmética son **codificaciones entrópicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergencia de Kullback-Leibler (KL)\n",
    "\n",
    "**Divergencia:** Medida de similitud entre distribuciones estadísticas (No es exactamente una distancia)\n",
    "\n",
    "Sean dos densidades $P=\\{p_1, p_2, \\ldots, p_N\\}$ y $Q=\\{q_1, q_2, \\ldots, q_N\\}$ sobre una misma variable $x$. Su **divergencia de KL** es\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{\\text{KL}} \\left ( P||Q \\right) &= \\sum_{i=1}^N p_i \\log_2 \\frac{p_i}{q_i} \\nonumber \\\\\n",
    "&= -\\sum_{i=1}^N p_i \\log_2 q_i + \\sum_{i=1}^N p_i \\log_2 p_i \\nonumber \\\\\n",
    "&= H_Q(P) -H(P) \\geq 0 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $H(P)$ es la entropía de $P$ \n",
    "- $H_Q(P)$ es la **entropía cruzada** de $P$ con $Q$\n",
    "- La desigualdad final se conoce como [desigualdad de Gibbs](https://en.wikipedia.org/wiki/Gibbs%27_inequality)\n",
    "- La igualdad se cumple sólo si $P=Q$\n",
    "\n",
    "La divergencia de KL también se conoce como la **entropía relativa de P con respecto a Q**\n",
    "\n",
    "\n",
    "> La divergencia de KL nos dice la \"cantidad extra\" de bits necesarios para codificar P usando un código optimizado para Q\n",
    "\n",
    "> La divergencia de KL mide cantidad de información perdida cuando uso Q para aproximar P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Información Mutua\n",
    "\n",
    "La información mutua entre dos variables aleatorias $X$ e $Y$ se puede definir de varias maneras\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MI}(X, Y) &= H(X) + H(Y) - H(X, Y)  \\nonumber \\\\\n",
    "&= H(Y) - H(X|Y) = H(X) - H(Y|X)   \\nonumber \\\\\n",
    "&= D_\\text{KL} \\left ( P_{XY} || P_{X} P_{Y} \\right) \\nonumber \\\\\n",
    "&= \\sum_{i} \\sum_{j} P_{XY}(X=x_i, Y=y_j) \\log_2 \\frac{P_{XY}(X=x_i,Y=y_j)}{P_{X}(X=x_i) P_Y(Y=y_j)}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $H(X,Y)$ es la entropía conjunta\n",
    "    - Cantidad de información promedio en bits de $X$ e $Y$\n",
    "- $H(X|Y)$ es la entropía de $X$ condicionada en $Y$\n",
    "    - Cantidad de información promedio en bits de $X$ considerando que conocemos $Y$\n",
    "    - Incerteza de $X$ dado que observamos $Y$\n",
    "    - Recordemos la relación entre densidad conjunta y condicional: $p(x|y)p(y) = p(x,y)$\n",
    "- Se cumple que $H(X)+H(Y) \\geq H(X,Y)$ por lo tanto $\\text{MI}(X,Y) \\geq 0$\n",
    "- Si $X$ e $Y$ son independientes entonces $H(X|Y)=H(X)$ y $H(X,Y) = H(X) + H(Y)$ y $\\text{MI}(X,Y) = 0$\n",
    "\n",
    "> La información mutua mide la información compartida por $X$ e $Y$, es decir que tan dependientes son entre sí\n",
    "\n",
    "> La información mutua nos dice la información promedio en bits que ganamos sobre $Y$ dado que observamos $X$ (y viceverza)\n",
    "\n",
    "> La información mutua nos dice la incerteza promedio de $Y$ que eliminamos al conocer $X$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canal con ruido\n",
    "\n",
    "Hasta ahora hemos asumido que el canal está libre de ruido\n",
    "\n",
    "El objetivo original de Shannon era\n",
    "\n",
    "> \"Comunicación robusta a través de un canal ruidoso\"\n",
    "\n",
    "El ruido disminuye la **capacidad** del canal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Un cisne navegando por  un canal ruidoso\n",
    "\n",
    "Imagemos que queremos transmitir una imagen binaria $X$ por un canal con ruido\n",
    "\n",
    "El canal le cambia el valor a un 10% de los píxeles\n",
    "\n",
    "Llamamos $Y$ a la imagen que sale del canal\n",
    "\n",
    "Este canal de comunicación se conoce como [canal binario simétrico](https://en.wikipedia.org/wiki/Binary_symmetric_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "th_binary, p_noise = 0.5, 0.1\n",
    "img_swan_gray = plt.imread('images/gray-swan.png')[:, :, 0]\n",
    "img_swan = (img_swan_gray > th_binary).astype('uint8')\n",
    "Npix = len(img_swan.ravel())\n",
    "p = np.random.rand(img_swan.shape[0], img_swan.shape[1])\n",
    "img_noisy_swan = img_swan.copy()\n",
    "img_noisy_swan[p <= p_noise] = 1-img_noisy_swan[p <= p_noise]\n",
    "\n",
    "ax[0].imshow(img_swan); ax[0].axis('off')\n",
    "ax[1].imshow(img_noisy_swan); ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la entropía de cada imagen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_binary_image(img):\n",
    "    p = np.count_nonzero(img.ravel())/len(img.ravel())\n",
    "    return -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "\n",
    "HX = entropy_binary_image(img_swan)\n",
    "HY = entropy_binary_image(img_noisy_swan)\n",
    "print(\"Entropía imagen limpia H(X): {0:0.4f} [bits/pixel]\".format(HX))\n",
    "print(\"Entropía imagen sucia H(Y): {0:0.4f} [bits/pixel]\".format(HY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la entropía conjunta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Npix = len(img_swan.ravel())\n",
    "p00 = np.count_nonzero((img_swan == 0) & (img_noisy_swan == 0))/Npix\n",
    "p01 = np.count_nonzero((img_swan == 0) & (img_noisy_swan == 1))/Npix\n",
    "p10 = np.count_nonzero((img_swan == 1) & (img_noisy_swan == 0))/Npix\n",
    "p11 = np.count_nonzero((img_swan == 1) & (img_noisy_swan == 1))/Npix\n",
    "print(np.array([p00, p01, p10, p11]))\n",
    "\n",
    "HXY = -(p00*np.log2(p00) + p01*np.log2(p01) + p10*np.log2(p10) + p11*np.log2(p11))\n",
    "print(\"Entropía conjunta H(X,Y): {0:0.6f} [bits/pixel]\".format(HXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las entropía condicionales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H(X|Y): {0:0.6f} [bits/pixel]\".format(HXY-HY))\n",
    "print(\"H(Y|X): {0:0.6f} [bits/pixel]\".format(HXY-HX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la información mutua?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXY = HX + HY - HXY\n",
    "print(\"Información mutua IM(X,Y): {0:0.6f} [bits/pixel]\".format(MIXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cúal es la entropía del ruido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noise = -p_noise*np.log2(p_noise) - (1-p_noise)*np.log2(1-p_noise)\n",
    "print(\"Entropía del ruido H(N): {0:0.6f} [bits/pixel]\".format(H_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos que $Y = X + N$\n",
    "\n",
    "Entonces\n",
    "$$\n",
    "H(Y|X) = H(X+N|X) = H(N|X) = H(N)\n",
    "$$\n",
    "\n",
    "> En un canal ruidoso la entropía condicional de la salida dada la entrada es equivalente a la entropía del ruido añadido a la entrada\n",
    "\n",
    "<img src=\"images/entropy_mi_diagram.png\">\n",
    "\n",
    "La eficiencia de la transmisión está dada por la proporción de entropía de $Y$ que es compartida por $X$\n",
    "\n",
    "$$\n",
    "E = \\frac{\\text{MI}(X,Y)}{H(Y)} \\in [0, 1]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXY/HY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un 37% de la entropía de la salida depende de la entrada\n",
    "\n",
    "¿Qué ocurre con la información mutua y con la eficiencia de transmisión cuando el canal se vuelve más o menos ruidoso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrección de errores debidos al ruido\n",
    "\n",
    "Queremos ser capaces de recuperar X a partir de Y\n",
    "\n",
    "Cuando el canal tiene ruido necesitamos robustecer el código de X\n",
    "\n",
    "Esto se logra **agregando redundancia** a nuestro código\n",
    "\n",
    "- Enviar el mensaje varias veces: **repetition code**\n",
    "- Agregar al código uno o más **bits de paridad**\n",
    "\n",
    "\n",
    "### Ejemplo: *repetition code*\n",
    "\n",
    "Si queremos enviar 0110011 lo repetimos una cierta cantidad de veces\n",
    "\n",
    "$X$ = 000 111 111 000 000 111 111 \n",
    "\n",
    "$N$ = 001 000 000 000 000 110 000\n",
    "\n",
    "$Y$ = 001 111 111 000 001 001 111\n",
    "\n",
    "Si aplicamos decodificación por mayoría: 0 1 1 0 0 **0** 1\n",
    "\n",
    "Reducimos la probabilidad de error por un factor 3\n",
    "\n",
    "La tasa es $R = k/n = 3$ donde $k$ son los bits de información y $n$ los bits transmitidos\n",
    "\n",
    "### Ejemplo: paridad\n",
    "\n",
    "Sea una tira binaria de 16 bits $s=[0,1,0,1,0,0,0,0,1,1,1,0, 0,1,1,0]$ Se ordena como una matriz de 4x4\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c c c c|c}\n",
    "  0 & 1 & 0 & 1 & 0\\\\\n",
    "  0 & 0 & 0 & 0 & 0\\\\\n",
    "  1 & 1 & 1 & 0 & 1\\\\\n",
    "  0 & 1 & 1 & 0 & 0 \\\\ \\hline\n",
    "  1 & 1 & 0 & 1 & \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Si el número de 1's de una fila o columna es par se agrega un 0, de lo contrario se agrega un 1\n",
    "\n",
    "Finalmente se crea una nueva tira de largo 24\n",
    "\n",
    "$s_p=[0,1,0,1,\\textbf{0}, 0,0,0,0,\\textbf{0}, 1,1,1,0, \\textbf{1}, 0,1,1,0, \\textbf{0}, \\textbf{1}, \\textbf{1}, \\textbf{0}, \\textbf{1}]$ \n",
    "\n",
    "- Al recibir este código se comprueba que las paridades estén bien\n",
    "- Si no lo están podriamos pedir nuevamente la tira binaria\n",
    "- Aumentamos el mensaje de 16 a 24 [bits], la tasa es $R=16/24=0.666$\n",
    "\n",
    "**Ejercicio:** Si tengo un string de NxN de largo y quiero proteger con bits de paridad ¿Cúantos bits agrego?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teorema de codificación de canal de Shannon (*Channel coding theorem*)\n",
    "\n",
    "Se define la capacidad de un canal con entrada $X$ y salida $Y$ como\n",
    "\n",
    "$$\n",
    "C = \\max_{P(X)} \\text{MI}(X,Y) ~\\text{[bits/símbolo]} \n",
    "$$\n",
    "\n",
    "La distribución $P^*(X)$ que maximiza la capacidad del canal es la distribución de entrada óptima para ese canal\n",
    "\n",
    "Notemos que si el canal no tiene ruido entonces $Y=X$ y la capacidad está dada por la entropía de $X$\n",
    "\n",
    "> El ruido disminuye la capacidad de un canal\n",
    "\n",
    "Al respecto Shannon enunció el siguiente teorema\n",
    "\n",
    "> Sea un canal con capacidad $C$ y una fuente $X$ que transmite a una taza de $R$\n",
    "\n",
    ">Si $R \\leq C$ entonces existe un un largo de codificación para $X$ que puede transmitirse con error arbitrariamente pequeño\n",
    "\n",
    "> Para una una probabilidad de error de bit aceptable $p_e$, se puede alcanzar una taza de transmisión\n",
    "$$\n",
    "R(p_e) = \\frac{1}{1 + p_e \\log_2(p_e) + (1-p_e) \\log_2(1-p_e)}\n",
    "$$\n",
    "\n",
    "> Para un cierto $p_e$ no es posible alcanzar una taza mayor a $R(p_e)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Canal binario simétrico\n",
    "\n",
    "¿Cuál es la distribución de entrada que maximiza la información mutua del canal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_flip, tol = 0.1, 1e-2\n",
    "np.random.seed(0)\n",
    "seed_image = np.random.rand(img_swan_gray.shape[0], img_swan_gray.shape[1])\n",
    "flip_mask = np.random.rand(img_swan_gray.shape[0], img_swan_gray.shape[1]) <= p_flip\n",
    "\n",
    "binarization_threshold = np.linspace(0.01, 0.999, num=100)\n",
    "MIXY = []\n",
    "for th in binarization_threshold:\n",
    "    img_X = (img_swan_gray > th).astype('uint8')\n",
    "    img_Y = img_X.copy()\n",
    "    img_Y[flip_mask] = 1-img_Y[flip_mask] \n",
    "    p00 = np.count_nonzero((img_X == 0) & (img_Y == 0))/Npix\n",
    "    p01 = np.count_nonzero((img_X == 0) & (img_Y == 1))/Npix\n",
    "    p10 = np.count_nonzero((img_X == 1) & (img_Y == 0))/Npix\n",
    "    p11 = np.count_nonzero((img_X == 1) & (img_Y == 1))/Npix\n",
    "    HX = -(p00 + p01)*np.log2(p00 + p01+tol) -(p10 + p11)*np.log2(p10 + p11+tol)\n",
    "    HY = -(p00 + p10)*np.log2(p00 + p10+tol) -(p01 + p11)*np.log2(p01 + p11+tol)\n",
    "    HXY= -(p00*np.log2(p00+tol) + p01*np.log2(p01+tol) + p10*np.log2(p10+tol) + p11*np.log2(p11+tol))\n",
    "    MIXY.append(HX + HY - HXY)\n",
    "    \n",
    "fig, ax = plt.subplots(2, 2, figsize=(9, 7), tight_layout=True)\n",
    "ax[1, 0].axis('off'); ax[0, 1].axis('off'); ax[1, 1].set_xticks([0, 1]); \n",
    "ax[1,1].set_xlabel('X'); ax[1, 1].set_ylabel('P(X)');\n",
    "ax[0, 0].set_ylabel('Información mutua')\n",
    "ax[0, 0].set_xlabel('Umbral de binarización');\n",
    "best_th = binarization_threshold[np.argmax(MIXY)]\n",
    "ax[0, 1].text(0.0,0.5, \"Mejor umbral: {0:0.4f}\\nMejor MI(X,Y): {1:0.4f}\".format(best_th, np.amax(MIXY)))\n",
    "ax[0, 0].plot(binarization_threshold, MIXY); \n",
    "ax[1, 0].imshow((img_swan_gray > best_th).astype('uint8')); \n",
    "ax[1, 1].hist((img_swan_gray > best_th).astype('uint8').ravel(), \n",
    "              align='mid', range=[-0.5, 1.5],\n",
    "              bins=2, density=True, histtype='bar', ec='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que la entrada $X$ tiene probabilidad $p$ de ser 0 y $(1-p)$ de ser 1 (imagen binaria) \n",
    "\n",
    "Es decir las *probabilidades a priori* son $P(X=0)=p$, $P(X=1)=(1-p)$ \n",
    "\n",
    "Dijimos que el canal cambia un 10\\% de los pixeles de $X$\n",
    "\n",
    "Entonces las verosimilitudes son $P(Y=1|X=1) = 0.9$, $P(Y=1|X=0) = 0.1$, $P(Y=0|X=1) = 0.1$, $P(Y=0|X=0) = 0.9$\n",
    "\n",
    "Las probabilidades marginales de la salida son:\n",
    "- $P(Y=1) = \\sum_x P(Y=1|X=x)P(X=x)  = 0.9(1-p) + 0.1p = 0.9 - 0.8p $\n",
    "- $P(Y=0) = \\sum_x P(Y=0|X=x)P(X=x)  = 0.1(1-p) + 0.9p = 0.1 + 0.8p= 1 - P(Y=1) $ \n",
    "\n",
    "y su entropía es $H(Y) = - (0.9 - 0.8p) \\log_2(0.9 - 0.8p) - (0.1 + 0.8p) \\log_2(0.1 + 0.8p)$\n",
    "\n",
    "\n",
    "La entropía condicional es \n",
    "$$\n",
    "H(Y|X) = \\sum_x H(Y|X=x)P(X=x) = - 0.9 \\log_2(0.9) - 0.1 \\log_2 (0.1)\n",
    "$$\n",
    "que no depende de $p$\n",
    "\n",
    "El máximo de la información mutua:\n",
    "$$\n",
    "\\frac{d}{dp} \\text{MI}(X,Y) = \\frac{d}{dp} H(Y) - \\frac{d}{dp} H(Y|X) = \\frac{d}{dp} H(Y) = 0 \\rightarrow p^*=\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Notemos que no importa que porcentaje de corrupción tenga el canal.\n",
    "\n",
    "Finalmente la capacidad del canal es\n",
    "\n",
    "$$\n",
    "C = \\text{MI}_p^* (X,Y) = 1 + 0.9 \\log_2(0.9) + 0.1 \\log_2 (0.1) = 0.531 \\text{[bits]}\n",
    "$$\n",
    "\n",
    "> Si usamos P^* podemos transmitir sin perdidas a una taza $R=k/n=0.531$, es decir que por $k$ bits de información tenemos que transmitir aproximadamente $2k$ bits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Teorema de Shannon-Hartley\n",
    "\n",
    "- Sea un canal con ancho de banda B [Hz] (rango de frecuencias que un canal puede transmitir) y potencia de señal S [W] y potencia del ruido N [W] (aditivo blanco gaussiano), entonces su capacidad es\n",
    "\n",
    "$$\n",
    "C = B \\log_2 \\left(1 + \\frac{S}{N} \\right) \\text{[bits/s]}\n",
    "$$\n",
    "\n",
    "- Si la velocidad de transmisión de un canal es R [bits/s] y R < C entonces la probabilidad de errores de comunicación tiende a cero.\n",
    "- Las limitaciones de un sistema de comunicación son **Ancho de banda** y el ruido **Ruido**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "http://home.iitk.ac.in/~adrish/Shannon/information_theory.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
