{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoría de la Información - Parte 2\n",
    "\n",
    "En esta lección:\n",
    "\n",
    "- Información Mutua\n",
    "- Codificación de canal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información Mutua\n",
    "\n",
    "La información mutua entre dos variables aleatorias $X$ e $Y$ se puede definir de varias maneras\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MI}(X, Y) &= H(X) + H(Y) - H(X, Y)  \\nonumber \\\\\n",
    "&= H(Y) - H(X|Y) \\nonumber \\\\ \n",
    "&= H(X) - H(Y|X)  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$H(X,Y)$ es la entropía conjunta \n",
    "> Mide  la cantidad de información promedio en bits de $X$ e $Y$\n",
    "\n",
    "$H(X|Y)$ es la entropía de $X$ condicionada en $Y$\n",
    "> Cantidad de información promedio en bits de $X$ considerando que conocemos $Y$ o también la \"Incerteza de $X$ dado que observamos $Y$\"\n",
    "\n",
    "Se cumple que $H(X)+H(Y) \\geq H(X,Y)$ por lo tanto $\\text{MI}(X,Y) \\geq 0$\n",
    "\n",
    "Si $X$ e $Y$ son independientes entonces $H(X|Y)=H(X)$ y $H(X,Y) = H(X) + H(Y)$ y $\\text{MI}(X,Y) = 0$\n",
    "\n",
    "> La información mutua mide la información compartida por $X$ e $Y$, es decir que tan dependientes son entre sí\n",
    "\n",
    "La información mutua nos dice \n",
    "\n",
    "- la información promedio en bits que ganamos sobre $Y$ dado que observamos $X$ (y viceverza)\n",
    "- la incerteza promedio de $Y$ que eliminamos al conocer $X$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canal con ruido\n",
    "\n",
    "Hasta ahora hemos asumido que el canal está libre de ruido\n",
    "\n",
    "El objetivo original de Shannon era\n",
    "\n",
    "> \"Comunicación robusta a través de un canal ruidoso\"\n",
    "\n",
    "El ruido disminuye la **capacidad** del canal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** Un cisne navegando por  un canal ruidoso\n",
    "\n",
    "Imagemos que queremos transmitir una imagen binaria $X$ por un canal con ruido. Asumamos que el canal le cambia el valor a un 10% de los píxeles y llamemos $Y$ a la imagen que sale del canal. ¿Cómo afecta esto la información mutua entre $X$ e $Y$?\n",
    "\n",
    "```{note}\n",
    "Este canal de comunicación se conoce como [canal binario simétrico](https://en.wikipedia.org/wiki/Binary_symmetric_channel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "th_binary, p_noise = 0.5, 0.1\n",
    "img_swan_gray = plt.imread('../images/gray-swan.png')[:, :, 0]\n",
    "img_swan = (img_swan_gray > th_binary).astype('uint8')\n",
    "Npix = len(img_swan.ravel())\n",
    "p = np.random.rand(img_swan.shape[0], img_swan.shape[1])\n",
    "img_noisy_swan = img_swan.copy()\n",
    "mask = p <= p_noise\n",
    "img_noisy_swan[mask] = 1-img_noisy_swan[mask]\n",
    "\n",
    "ax[0].imshow(img_swan); ax[0].axis('off')\n",
    "ax[1].imshow(img_noisy_swan); ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la entropía de cada imagen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_binary_image(img):\n",
    "    p = np.count_nonzero(img.ravel())/len(img.ravel())\n",
    "    return -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "\n",
    "HX = entropy_binary_image(img_swan)\n",
    "HY = entropy_binary_image(img_noisy_swan)\n",
    "print(\"Entropía imagen limpia H(X): {0:0.4f} [bits/pixel]\".format(HX))\n",
    "print(\"Entropía imagen sucia H(Y): {0:0.4f} [bits/pixel]\".format(HY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la entropía conjunta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Npix = len(img_swan.ravel())\n",
    "p00 = np.count_nonzero((img_swan == 0) & (img_noisy_swan == 0))/Npix\n",
    "p01 = np.count_nonzero((img_swan == 0) & (img_noisy_swan == 1))/Npix\n",
    "p10 = np.count_nonzero((img_swan == 1) & (img_noisy_swan == 0))/Npix\n",
    "p11 = np.count_nonzero((img_swan == 1) & (img_noisy_swan == 1))/Npix\n",
    "print(np.array([p00, p01, p10, p11]))\n",
    "\n",
    "HXY = -(p00*np.log2(p00) + p01*np.log2(p01) + p10*np.log2(p10) + p11*np.log2(p11))\n",
    "print(\"Entropía conjunta H(X,Y): {0:0.6f} [bits/pixel]\".format(HXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las entropía condicionales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H(X|Y): {0:0.6f} [bits/pixel]\".format(HXY-HY))\n",
    "print(\"H(Y|X): {0:0.6f} [bits/pixel]\".format(HXY-HX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cual es la información mutua?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXY = HX + HY - HXY\n",
    "print(\"Información mutua IM(X,Y): {0:0.6f} [bits/pixel]\".format(MIXY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cúal es la entropía del ruido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_noise = -p_noise*np.log2(p_noise) - (1-p_noise)*np.log2(1-p_noise)\n",
    "print(\"Entropía del ruido H(N): {0:0.6f} [bits/pixel]\".format(H_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos que $Y = X + N$\n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\n",
    "H(Y|X) = H(X+N|X) = H(N|X) = H(N)\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "En un canal ruidoso la entropía condicional de la salida dada la entrada es equivalente a la entropía del ruido añadido a la entrada\n",
    "```\n",
    "\n",
    "<img src=\"../images/entropy_mi_diagram.png\" width=\"500\">\n",
    "\n",
    "La eficiencia de la transmisión está dada por la proporción de entropía de $Y$ que es compartida por $X$\n",
    "\n",
    "$$\n",
    "E = \\frac{\\text{MI}(X,Y)}{H(Y)} \\in [0, 1]\n",
    "$$\n",
    "\n",
    "Para este caso tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXY/HY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir que un 37% de la entropía de la salida depende de la entrada\n",
    "\n",
    "```{admonition} Pregunta\n",
    ":class: tip\n",
    "¿Qué ocurre con la información mutua y con la eficiencia de transmisión cuando el canal se vuelve más o menos ruidoso?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección de errores debidos al ruido\n",
    "\n",
    "Queremos ser capaces de recuperar X a partir de Y\n",
    "\n",
    "Cuando el canal tiene ruido necesitamos robustecer el código de X\n",
    "\n",
    "Esto se logra agregando **redundancia** a nuestro código\n",
    "\n",
    "- Enviar el mensaje varias veces: **repetition code**\n",
    "- Agregar al código uno o más **bits de paridad**\n",
    "\n",
    "\n",
    "**Ejemplo 1:** *repetition code*\n",
    "\n",
    "Si queremos enviar 0110011 lo repetimos una cierta cantidad de veces\n",
    "\n",
    "$X$ = 000 111 111 000 000 111 111 \n",
    "\n",
    "$N$ = 001 000 000 000 000 110 000\n",
    "\n",
    "$Y$ = 001 111 111 000 001 001 111\n",
    "\n",
    "Si aplicamos decodificación por mayoría: 0 1 1 0 0 **0** 1\n",
    "\n",
    "- Reducimos la probabilidad de error por un factor de 3 \n",
    "- Reducimos la tasa de transmisión en un factor de 3\n",
    "\n",
    "La tasa de transmisión es $R = k/n = 1/3$ donde $k$ son los bits de información y $n$ los bits transmitidos\n",
    "\n",
    "**Ejemplo 2:** paridad\n",
    "\n",
    "Sea una tira binaria de 16 bits $s=[0,1,0,1,0,0,0,0,1,1,1,0, 0,1,1,0]$ Se ordena como una matriz de 4x4\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c c c c|c}\n",
    "  0 & 1 & 0 & 1 & 0\\\\\n",
    "  0 & 0 & 0 & 0 & 0\\\\\n",
    "  1 & 1 & 1 & 0 & 1\\\\\n",
    "  0 & 1 & 1 & 0 & 0 \\\\ \\hline\n",
    "  1 & 1 & 0 & 1 & \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Si el número de 1's de una fila o columna es par se agrega un 0, de lo contrario se agrega un 1\n",
    "\n",
    "Finalmente se crea una nueva tira de largo 24\n",
    "\n",
    "$s_p=[0,1,0,1,\\textbf{0}, 0,0,0,0,\\textbf{0}, 1,1,1,0, \\textbf{1}, 0,1,1,0, \\textbf{0}, \\textbf{1}, \\textbf{1}, \\textbf{0}, \\textbf{1}]$ \n",
    "\n",
    "- Al recibir este código se comprueba que las paridades estén bien\n",
    "- Si no lo están podriamos pedir nuevamente la tira binaria\n",
    "- Aumentamos el mensaje de 16 a 24 [bits], la tasa es $R=16/24=0.666$\n",
    "\n",
    "**Ejercicio propuesto:** \n",
    "\n",
    "Si tengo un string de NxN de largo y quiero proteger con bits de paridad ¿Cúantos bits agrego?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teorema de codificación de canal de Shannon (*Channel coding theorem*)\n",
    "\n",
    "Se define la capacidad de un canal con entrada $X$ y salida $Y$ como\n",
    "\n",
    "$$\n",
    "C = \\max_{P(X)} \\text{MI}(X,Y) ~\\text{[bits/símbolo]} \n",
    "$$\n",
    "\n",
    "La distribución $P^*(X)$ que maximiza la capacidad del canal es la distribución de entrada óptima para ese canal. Si el canal no tuviera ruido entonces $Y=X$ y la capacidad está dada por la entropía de $X$. El ruido disminuye la capacidad de un canal\n",
    "\n",
    "Al respecto Shannon enunció el siguiente teorema\n",
    "\n",
    "> Sea un canal con capacidad $C$ y una fuente $X$ que transmite a una tasa de $R$\n",
    "\n",
    ">Si $R \\leq C$ entonces existe un largo de codificación para $X$ que puede transmitirse con error arbitrariamente pequeño\n",
    "\n",
    "> Para una probabilidad de error de bit aceptable $p_e$, se puede alcanzar una tasa de transmisión\n",
    "\n",
    "$$\n",
    "R(p_e) = \\frac{1}{1 + p_e \\log_2(p_e) + (1-p_e) \\log_2(1-p_e)}\n",
    "$$\n",
    "\n",
    "> Para un cierto $p_e$ no es posible alcanzar una tasa mayor a $R(p_e)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** Canal binario simétrico\n",
    "\n",
    "¿Cuál es la distribución de entrada que maximiza la información mutua del canal?\n",
    "\n",
    "Respondamos en primer lugar usando Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_flip, tol = 0.1, 1e-2\n",
    "np.random.seed(0)\n",
    "seed_image = np.random.rand(img_swan_gray.shape[0], img_swan_gray.shape[1])\n",
    "flip_mask = np.random.rand(img_swan_gray.shape[0], img_swan_gray.shape[1]) <= p_flip\n",
    "\n",
    "binarization_threshold = np.linspace(0.01, 0.999, num=100)\n",
    "MIXY = []\n",
    "for th in binarization_threshold:\n",
    "    # Aplicamos el umbral de binarización\n",
    "    img_X = (img_swan_gray > th).astype('uint8')\n",
    "    img_Y = img_X.copy()\n",
    "    # Simulamos la perturbación del canal\n",
    "    img_Y[flip_mask] = 1 - img_Y[flip_mask] \n",
    "    # Calculamos las entropías y la IM\n",
    "    p00 = np.count_nonzero((img_X == 0) & (img_Y == 0))/Npix\n",
    "    p01 = np.count_nonzero((img_X == 0) & (img_Y == 1))/Npix\n",
    "    p10 = np.count_nonzero((img_X == 1) & (img_Y == 0))/Npix\n",
    "    p11 = np.count_nonzero((img_X == 1) & (img_Y == 1))/Npix\n",
    "    HX = -(p00 + p01)*np.log2(p00 + p01+tol) -(p10 + p11)*np.log2(p10 + p11+tol)\n",
    "    HY = -(p00 + p10)*np.log2(p00 + p10+tol) -(p01 + p11)*np.log2(p01 + p11+tol)\n",
    "    HXY= -(p00*np.log2(p00+tol) + p01*np.log2(p01+tol) + p10*np.log2(p10+tol) + p11*np.log2(p11+tol))\n",
    "    MIXY.append(HX + HY - HXY)\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3), tight_layout=True) \n",
    "best_th = binarization_threshold[np.argmax(MIXY)]\n",
    "ax.plot(binarization_threshold, MIXY); \n",
    "ax.axhline(np.amax(MIXY), c='k', ls='--')\n",
    "ax.axvline(binarization_threshold[np.argmax(MIXY)], c='k', ls='--')\n",
    "ax.set_ylabel('Información mutua')\n",
    "ax.set_xlabel('Umbral de binarización');\n",
    "display(f\"El mejor umbral es {binarization_threshold[np.argmax(MIXY)]:0.4f}\",\n",
    "        f\"La IM alcanzada es {np.amax(MIXY):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen resultante de aplicar el mejor umbral de binarización y la distribución de sus píxeles se muestra a continuación\n",
    "\n",
    "Recordemos que mientras más uniforme es una distribución mayor es su entropía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_binary_image = (img_swan_gray > binarization_threshold[np.argmax(MIXY)]).astype('uint8')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "ax[0].imshow(best_binary_image); \n",
    "ax[0].axis('off'); \n",
    "ax[1].hist(best_binary_image.ravel(), align='mid', range=[-0.5, 1.5], \n",
    "           bins=2, density=True, histtype='bar', ec='black');\n",
    "ax[1].set_xlabel('Valor pixel binarizado');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora si podemos llegar a la misma solución de forma teórica\n",
    "\n",
    "Digamos que la imagen de entrada binaria se obtiene aplicando un umbral $p$, es decir que si el pixel original en escala de grises es mayor que $p$ entonces el resultado $1$ de lo contrario es $0$.\n",
    "\n",
    "Con esto podemos escribir las *probabilidades a priori* de cada pixel como\n",
    "\n",
    "- $P(X=0)=p$\n",
    "- $P(X=1)=(1-p)$ \n",
    "\n",
    "Dijimos que el canal cambia un 10\\% de los pixeles de la entrada. Entonces las verosimilitudes son $P(Y=1|X=1) = 0.9$, $P(Y=1|X=0) = 0.1$, $P(Y=0|X=1) = 0.1$, $P(Y=0|X=0) = 0.9$\n",
    "\n",
    "Por ende las probabilidades marginales de la salida son:\n",
    "\n",
    "- $P(Y=1) = \\sum_x P(Y=1|X=x)P(X=x)  = 0.9(1-p) + 0.1p = 0.9 - 0.8p $\n",
    "- $P(Y=0) = \\sum_x P(Y=0|X=x)P(X=x)  = 0.1(1-p) + 0.9p = 0.1 + 0.8p= 1 - P(Y=1) $ \n",
    "\n",
    "y su entropía es \n",
    "\n",
    "$$\n",
    "H(Y) = - (0.9 - 0.8p) \\log_2(0.9 - 0.8p) - (0.1 + 0.8p) \\log_2(0.1 + 0.8p)\n",
    "$$\n",
    "\n",
    "Por otro lado, la entropía condicional es \n",
    "\n",
    "$$\n",
    "H(Y|X) = \\sum_x H(Y|X=x)P(X=x) = - 0.9 \\log_2(0.9) - 0.1 \\log_2 (0.1),\n",
    "$$\n",
    "\n",
    "que no depende de $p$.\n",
    "\n",
    "Podemos calcular el máximo de la información mutua usando\n",
    "\n",
    "$$\n",
    "\\frac{d}{dp} \\text{MI}(X,Y) = \\frac{d}{dp} H(Y) - \\frac{d}{dp} H(Y|X) = \\frac{d}{dp} H(Y) = 0 \n",
    "$$\n",
    "\n",
    "de donde llegamos a que\n",
    "\n",
    "$$\n",
    "p^*=\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Notemos que no importa que porcentaje de corrupción tenga el canal.\n",
    "\n",
    "Finalmente la capacidad del canal es\n",
    "\n",
    "$$\n",
    "C = \\text{MI}_{p^*} (X,Y) = 1 + 0.9 \\log_2(0.9) + 0.1 \\log_2 (0.1) = 0.531 \\text{[bits]}\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "Si usamos $P^*$ podemos transmitir sin perdidas a una taza $R=k/n \\leq 0.531$, es decir que por $k$ bits de información tenemos que transmitir al menos $2k$ bits\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Teorema de Shannon-Hartley\n",
    "\n",
    "Sea un canal con ancho de banda B [Hz] (rango de frecuencias que un canal puede transmitir) y potencia de señal S [W] y potencia del ruido N [W] (aditivo blanco gaussiano), entonces su capacidad es\n",
    "\n",
    "$$\n",
    "C = B \\log_2 \\left(1 + \\frac{S}{N} \\right) \\text{[bits/s]}\n",
    "$$\n",
    "\n",
    "Si la velocidad de transmisión de un canal es R [bits/s] y R < C entonces la probabilidad de errores de comunicación tiende a cero. \n",
    "\n",
    "Las limitaciones de un sistema de comunicación son **Ancho de banda** y el ruido **Ruido**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
